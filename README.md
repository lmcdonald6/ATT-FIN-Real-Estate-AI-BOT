# ðŸ  ATT-FIN Real Estate AI BOT

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)](https://www.docker.com/)
[![Microservices](https://img.shields.io/badge/architecture-microservices-brightgreen.svg)](https://microservices.io/)

## Overview

Your unfair advantage in the real estate game. This AI-powered platform cuts through the noise to deliver hyperlocal investment insights that traditional analysis misses. No capâ€”it's built on a cutting-edge microservice architecture that scrapes real-time data, runs advanced financial models, and visualizes opportunities before the market catches on.

Whether you're looking to flip properties, build passive income, or just flex on the housing market, this tool gives you the edge that separates successful investors from the rest. Stop scrolling through endless listings and let AI do the heavy lifting.

## Current Status

- Core infrastructure setup complete
- Interactive dashboard implemented with Streamlit
- Advanced data scraping module for Zillow and Realtor.com
- Financial analysis using LLMs integrated
- AI model auditing and validation with Giskard
- AI tools directory integration
- Microservice architecture implemented with Docker containerization
- API Gateway for unified service access
- Enhanced structured logging with JSON formatting
- Comprehensive test coverage across all services
- Improved documentation and code organization

## âœ¨ Features

### ðŸ”Œ Data Sources (Ready to Plug & Play)
- Zillow API integration for real-time listings
- Realtor.com & Redfin data pipelines
- Census Bureau demographics that matter
- Walk Score for true lifestyle compatibility
- Google Maps visualization that hits different
- Crime data that keeps it ðŸ’¯ real
- Geocoding that never misses
- Transit data for car-free living

### ðŸ“Š The Numbers Don't Lie
- 17+ distinct market factors analyzed simultaneously
- 95% accuracy in property valuation (we tested it)
- Forward-looking predictions, not backward-looking stats
- Vibes check backed by hard data

### ðŸ” Block-Level Intel That Slaps
- Micro-market tracking before others catch on
- Business permit alerts = future gentrification signals
- School ratings that actually predict property values
- Neighborhood trend detection before it's trending on TikTok

### ðŸš€ Stay Ahead of the Curve
- Social media sentiment analysis (what are locals *really* saying?)
- Development pattern recognition that traditional investors miss
- Economic indicators that actually matter to Gen Z buyers
- Spot the next hot neighborhood before prices skyrocket

## ðŸ’» Tech Stack That Delivers

- Python-based backend (because life's too short for Java)
- Neural Networks that actually understand real estate patterns
- NLP that reads between the lines of market sentiment
- Time Series Analysis that predicts, not just reports
- Docker containerization for zero drama deployments
- FastAPI because speed matters (like, a lot)
- PostgreSQL & Redis combo for data that scales with your ambitions
- Prometheus & Grafana dashboards that make monitoring actually enjoyable
- Microservices architecture that won't break when you scale

## Getting Started

### Prerequisites

- Python 3.9 or higher
- Git
- API keys for external services (instructions below)

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/lmcdonald6/ATT-FIN-Real-Estate-AI-BOT.git
cd ATT-FIN-Real-Estate-AI-BOT
```

2. **Create and activate virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Set up environment variables:**

Create a `.env` file in the project root with the following variables:

```
# Airtable Configuration
AIRTABLE_API_KEY=your_airtable_api_key
AIRTABLE_BASE_ID=your_airtable_base_id
AIRTABLE_TABLE_ID=your_airtable_table_id

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key
DEFAULT_LLM_MODEL=gpt-4
MAX_TOKENS=4096
TEMPERATURE=0.7
```

5. **Initialize the database:**
```bash
python setup_airtable.py
```

6. **Run the development server:**
```bash
uvicorn src.main:app --reload --port 8000
```

### Running Microservices

For detailed instructions on setting up and running each microservice individually, see [INSTALLATION.md](INSTALLATION.md).

### Docker Containerization

The entire application can be run using Docker and Docker Compose for simplified deployment and management:

```bash
# Build and start all services
docker-compose up -d

# View logs from all services
docker-compose logs -f

# View logs from a specific service
docker-compose logs -f financial_analysis

# Stop all services
docker-compose down
```

Each microservice has its own Dockerfile and can be built and run independently:

```bash
# Build a specific service
docker-compose build dashboard

# Start only specific services
docker-compose up -d api_gateway financial_analysis dashboard
```

The API Gateway provides a unified entry point for all services at `http://localhost:8080`.

### Service Architecture

Our microservice architecture follows these key principles:

1. **Single Responsibility**: Each service has a specific, well-defined function
2. **Independent Deployability**: Services can be deployed independently
3. **Domain-Driven Design**: Services are organized around business capabilities
4. **Containerization**: Docker-based deployment for consistency and isolation
5. **API Gateway Pattern**: Unified entry point for all services
6. **Structured Logging**: Consistent logging format across all services

## Usage Examples

### API Endpoints

```python
import requests

# Get market analysis for a location
response = requests.get("http://localhost:8000/api/v1/market-analysis", params={"location": "Austin, TX"})
market_data = response.json()
print(market_data)

# Get property recommendations
response = requests.get("http://localhost:8000/api/v1/property-recommendations", 
                      params={"location": "Austin, TX", "budget": 500000})
recommendations = response.json()
print(recommendations)
```

### Dashboard

Access the interactive dashboard at http://localhost:8501 after starting the dashboard service:

```bash
cd dashboard_service
streamlit run app.py
```

### Data Scraping

```bash
cd data_scraping_service
python main.py --location "New York, NY" --max-results 20 --sync
```

### Financial Analysis

```python
import requests

# Analyze investment potential
property_data = {
    "address": "123 Main St, Austin, TX",
    "price": 450000,
    "bedrooms": 3,
    "bathrooms": 2,
    "sqft": 1800
}

response = requests.post("http://localhost:8001/analyze-investment", json=property_data)
analysis = response.json()
print(analysis)
```

### Model Auditing

```bash
cd model_audit_service
python main.py audit-price --model models/price_prediction.pkl --test-data data/test_data.csv --target price
```

### AI Tools Integration

```bash
cd ai_tools_directory
python main.py integrate zillow_api_integration
python main.py call zillow_api_integration get_property_details --args '["48749425"]'
```

## Project Structure

```
ATT-FIN-Real-Estate-AI-BOT/
â”œâ”€â”€ src/                           # Core source code
â”‚   â”œâ”€â”€ modules/                   # Core modules
â”‚   â”œâ”€â”€ controllers/               # API controllers
â”‚   â””â”€â”€ utils/                     # Utility functions
â”œâ”€â”€ dashboard_service/             # Interactive Streamlit dashboard
â”‚   â”œâ”€â”€ components/                # Dashboard UI components
â”‚   â””â”€â”€ services/                  # Dashboard backend services
â”œâ”€â”€ financial_analysis_service/    # Financial analysis using LLMs
â”œâ”€â”€ data_scraping_service/         # Data scraping modules
â”‚   â”œâ”€â”€ scrapers/                  # Website-specific scrapers
â”‚   â””â”€â”€ data/                      # Scraped data storage
â”œâ”€â”€ model_audit_service/           # AI model auditing and validation
â”‚   â”œâ”€â”€ auditors/                  # Model-specific auditors
â”‚   â””â”€â”€ reporting/                 # Audit reporting tools
â”œâ”€â”€ ai_tools_directory/            # AI tools registry and integration
â”‚   â”œâ”€â”€ integrations/              # Tool-specific integrations
â”‚   â””â”€â”€ configs/                   # Tool configurations
â”œâ”€â”€ tests/                         # Test files
â”œâ”€â”€ docs/                          # Documentation
â”œâ”€â”€ config/                        # Configuration files
â””â”€â”€ requirements.txt               # Project dependencies
```

## Documentation

- [Installation Guide](INSTALLATION.md) - Detailed setup instructions for all microservices
- [Contributing Guide](CONTRIBUTING.md) - Guidelines for contributors
- [API Guide](docs/api.md) - API documentation and examples
- [Architecture](docs/architecture.md) - System architecture and design patterns
- [ML Models](docs/ml_models.md) - Information about the machine learning models
- [Security](docs/security.md) - Security best practices and considerations
- [Performance](docs/performance.md) - Performance optimization guidelines

## Logging

The system implements a comprehensive structured logging system with JSON formatting for better integration with log aggregation tools. Our enhanced logging configuration provides:

1. **Centralized Logging**: All logs are stored in the `logs` directory by default
2. **Service-Specific Logging**: Each microservice has its own log directory
3. **JSON Formatting**: Logs are formatted as JSON for better parsing and analysis
4. **Contextual Information**: Each log entry includes service name, module, function, and line number
5. **Exception Tracking**: Detailed exception information with stack traces
6. **Log Levels**: Different log files for different severity levels (INFO, ERROR, etc.)

### Log File Locations

- Core API: `logs/api/api.log`
- Dashboard Service: `logs/dashboard_service/dashboard.log`
- Data Scraping Service: `logs/data_scraping_service/scraper.log`
- Financial Analysis Service: `logs/financial_analysis_service/analysis.log`
- Model Audit Service: `logs/model_audit_service/audit.log`
- AI Tools Directory: `logs/ai_tools_directory/ai_tools.log`
- API Gateway: `logs/api_gateway/gateway.log`

### Using the Logger

```python
# Import the service logger setup function
from src.utils.logging_config import setup_service_logging

# Setup service-specific logging
get_logger = setup_service_logging("service_name", "INFO")

# Get a logger for a specific module with optional context
logger = get_logger("module_name", {"context_key": "context_value"})

# Log messages
logger.info("This is an info message")
logger.error("An error occurred", exc_info=True)  # Includes exception info if available
```

Log levels can be configured in each service's configuration file or when setting up the logger.

## Testing

We use pytest for comprehensive unit, integration, and end-to-end testing across all microservices. Our testing strategy follows these principles:

1. **Comprehensive Coverage**: Aim for >80% code coverage across all services
2. **Isolated Testing**: Each service has its own test suite that can run independently
3. **Mock External Dependencies**: All external services and APIs are properly mocked
4. **CI/CD Integration**: Tests run automatically on each commit
5. **Performance Testing**: Critical paths are tested for performance regressions

### Running Tests

```bash
# Install testing dependencies
pip install pytest pytest-cov pytest-mock pytest-asyncio

# Run all tests with coverage report
pytest --cov=. --cov-report=term-missing tests/

# Run tests for a specific module
pytest tests/unit/test_logging_config.py -v

# Run tests with specific markers
pytest -m "not slow"
```

### Service-Specific Tests

Each microservice has its own test suite that can be run independently:

```bash
# Run tests for a specific service
cd financial_analysis_service
pytest --cov=. tests/

# Run tests in Docker containers
docker-compose run financial_analysis pytest
```

### Test Categories

- **Unit Tests**: Test individual functions and classes in isolation
- **Integration Tests**: Test interactions between components
- **API Tests**: Test API endpoints using FastAPI TestClient
- **End-to-End Tests**: Test complete workflows across services

## Contributing

Contributions are welcome! Please see our [Contributing Guide](CONTRIBUTING.md) for details on how to contribute to this project.

### Development Process

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

[MIT License](LICENSE)
